# ğŸ§¬ Protein Fold and Secondary Structure Prediction using ProtBERT, Machine Learning, and Quantum Hybrid Models

This project explores **Protein Fold Prediction** and **Protein Secondary Structure Prediction (PSSP)** using advanced techniques such as **transformer-based embeddings (ProtBERT)**, classical **supervised machine learning models**, and a **Quantum-Classical Hybrid MLP architecture**. 

It aims to classify proteins into distinct fold types and predict structural elements like **alpha-helices** and **beta-sheets**, offering insights into their biological functions.

---

## ğŸ“Œ Key Features

- âš›ï¸ **ProtBERT-based Embedding** for protein sequence representation.
- ğŸ§  **Machine Learning Models** for classification of protein folds and secondary structure.
- ğŸ”¬ **Quantum Hybrid MLP** to integrate quantum computing for enhanced prediction.
- ğŸ“Š Comprehensive evaluation using **accuracy**, **precision**, **recall**, and **F1-score**.
- ğŸ§¼ Effective preprocessing of **CASP datasets** for PSSP.
- ğŸ“ˆ Feature selection and dimensionality reduction for improved performance.

---

## ğŸ§± Architecture Overview

### 1ï¸âƒ£ Protein Fold Prediction

- **Input**: Raw protein sequences.
- **Embedding**: `ProtBERT` transforms sequences into 1024-dimensional embeddings.
- **Models Used**:
  - Single-layer MLP (Input â†’ 100 neurons â†’ Output)
  - Multi-layer MLP (Input â†’ 128 â†’ 4 â†’ Output)
  - Hybrid MLP (Input â†’ 128 classical â†’ 4 quantum qubits â†’ Output)
- **Quantum Component**: The Hybrid MLP integrates a quantum circuit using 4 qubits to capture complex patterns beyond classical capabilities.

### 2ï¸âƒ£ Protein Secondary Structure Prediction (PSSP)

- **Input**: CASP dataset (after cleaning and formatting).
- **Feature Engineering**: Automated using ProtBERT + manual selection for key features.
- **Prediction**: Classification into secondary structure types (e.g., Î±-helix, Î²-sheet).
- **Evaluation**: Performance assessed using standard classification metrics.

---

## ğŸ”„ Data Preprocessing

- **ProtBERT Embedding**:
  - Transformer-based model trained on massive protein sequence data.
  - Learns amino acid relationships and encodes sequences into feature-rich vectors.
  - Automates feature engineering, replacing manual approaches.
  - Enhances training stability through normalization.

- **Preprocessing for PSSP**:
  - Data cleaning and formatting.
  - Feature selection to retain key structural signals while reducing dimensionality.

---

## ğŸ—ï¸ Model Architecture

### MLP Variants

| Model Type         | Layers/Neurons                   | Notes                                      |
|--------------------|----------------------------------|--------------------------------------------|
| Single-layer MLP   | Input: 1024 â†’ Hidden: 100 â†’ Out  | Basic model for pattern extraction         |
| Multi-layer MLP    | Input â†’ 128 â†’ 4 â†’ Output         | Captures complex non-linear relationships |
| Hybrid MLP         | Input â†’ 128 (classical) + 4 Qubits â†’ Output | Quantum-enhanced feature learning |

---

## ğŸ“Š Evaluation Metrics

- **Accuracy**
- **Precision**
- **Recall**
- **F1-Score**

Each model is evaluated using these metrics to determine performance on both fold classification and secondary structure prediction tasks.

---

## ğŸš€ Getting Started

### ğŸ”§ Requirements

- Python 3.8+
- `transformers`, `scikit-learn`, `qiskit`, `pennylane`, `torch`, `numpy`, `matplotlib`

### ğŸ›  Installation

```bash
pip install -r requirements.txt
